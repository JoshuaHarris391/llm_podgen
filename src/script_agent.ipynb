{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BM25Retriever\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/\").load_data() \n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=llm)\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context, service_context=service_context\n",
    ")\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)\n",
    "\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import NodeWithScore\n",
    "from typing import List\n",
    "\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=4, model=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "def query_engine(query) -> List[NodeWithScore]:\n",
    "    # hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)\n",
    "    # response = hybrid_retriever.retrieve(query)\n",
    "    hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)\n",
    "    nodes = hybrid_retriever.retrieve(query)\n",
    "    reranked_nodes = reranker.postprocess_nodes(\n",
    "        nodes,\n",
    "        query_bundle=QueryBundle(query),\n",
    "    )\n",
    "\n",
    "    return [node.text for node in reranked_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Preprint. Under review.\\nOriginal Prompt LongLLMLingua\\nCompressed PromptInstruction: Answer the question based \\non the given passages. Only give me the \\nanswer and do not output any other \\nwords. The following are given \\npassages.\\nDocument 1: Alberic  III of Dammartin\\nAlberic  III of Dammartin  (Aubry de \\nDammartin ) (c.\\u20091138 – 19 September \\n1200) was a French count and son of \\nAlberic  II, Count of Dammartin , and \\nClé mence  de Bar, daughter of Reginald \\nI, Count of Bar…\\nDocument 2:\\n…\\nDocument N: Pope Agapetus II\\nPope Agapetus II (died 8 November 955) \\nwas the bishop of Rome and ruler of the \\nPapal States from 10 May 946 to his \\ndeath.',\n",
       " ':ivalent What is to hold the lens the the star:\\nWhy toason\\na for behavior , or that the accepted of:ivalent of Perg What religion What country you the\\nWhat does V:: Where I a goodboard for:: buyies on the the the: areter cookiespped with\\ncres: theoe thated ofasticitations , as ‘ the rules to “: the three What do for an:: CNN in:: is\\na:ose special bears was on17 the Who used Au an electionan: what book: is to the various\\nways can measure IT:chni and method is software What British minister and wereins: aic\\nthe to overcome fear What drink would the biggest:: the States do people longest:: which\\nthe the rare disease as : , andentizations , , and is of a is What Russian mastery What\\na perfect a: What c was Thomas in: Other: did the of What did What can feature the\\ndifferent:ques the-O the ons lips at What anetic did Victoria used her child: D What do:\\nmany from to of ofors ,',\n",
       " 'Preprint. Under review.\\nCompressed Prompt:\\nPlease the of the question. questions\\nare sometimes your cold but the of you isnt:ason: What food hasges:: Who the first coach\\nthe Clevelandns What arch the Placede: Other: Who created Harryime What Carbean cult\\ndidvey:: did Iraqi troops::ose cover is of an of Universal Import What the of Betty theest\\nthectic:: Wh the founder and of The National Review:: was T Tims What the historicalals\\nfollowing the of Agra is whiteolate: of What the the: is a of everything:ase and:ose old\\nLondon come- was : “y my sweet:: The major team in is called: Group or organization\\nof: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k and\\nch: of: the lawyer for Randy C:: the Francisco What year the in whereci became What\\ncountry most is g the Who the to P What are the states the the name , Elino: What manmade\\nwaterways is1.',\n",
       " 'Elino: What manmade\\nwaterways is1.76: Other of Z:ivalent of: of What was the:: How do ants have: of: the Dow\\nfirst the high sound that hear in ear every then , but then it away ,:: didist control in:: How\\ncan I ofies ’ What did theramid-ers of Egypt eat:: How does Belle her inast: M of: When\\nreading classs does EENTY :: Expression abbre: When was Florida:: manyelies were killed\\nthe: Whative on Punchl Hill and has1 What the Filenes the cookies in Internet: What word\\ncontains: Word with a special is Larry: a person: a Frenchist: of What American wrote : “\\nGoodors:: Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is\\nof was Jean: What the2 What caused Harryini What buildingately enough the the1d bill:\\nOther location: many logmic there a rule:: the the word ,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing engine\n",
    "query_engine(\"What is the introduction?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen.agentchat import AssistantAgent\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "query_llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"query_engine\",\n",
    "            \"description\": \"An engine that answers queries on an scientific paper\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Use a detailed plain text sentence command as the input to the tool.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "    \"seed\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "    \"seed\": 2,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "paper_names = ['LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION']\n",
    "interviewee = AssistantAgent(\n",
    "    name=\"interviewee\",\n",
    "    system_message=f\"You are on a podcast show called 'Paper of the Day', being interviewed by the interviewer. To access your brain on the knowledge of the scientific paper, you should call the query_engine function to help you understand the paper. The papers you are talking about today are: {paper_names}. Your should always use the information from the query engine in your reply. Remember your interviewer and your audience has no domain knowledge so you need to keep the response simple and easy to digest. Keep your answer short, and more conversational.\",\n",
    "    llm_config=query_llm_config,\n",
    "    max_consecutive_auto_reply=10,\n",
    ")\n",
    "\n",
    "\n",
    "interviewer = AssistantAgent(\n",
    "    name=\"interviewer\",\n",
    "    system_message=\"You are the interviewer of today's show 'Paper of the Day', you will interview the interviewee on the scientific paper of the day. Your objective is to interview the interviewee to help the audience understand the content of the paper better. You do not know anything about the paper, make sure to keep asking questions in each response.\",\n",
    "    llm_config=query_llm_config,\n",
    "    max_consecutive_auto_reply=10,\n",
    ")\n",
    "\n",
    "# register the functions\n",
    "interviewee.register_function(\n",
    "    function_map={\n",
    "        \"query_engine\": query_engine,\n",
    "    }\n",
    ")\n",
    "\n",
    "# register the functions\n",
    "interviewer.register_function(\n",
    "    function_map={\n",
    "        \"query_engine\": query_engine,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "Thank you for having me on the show, what do you want to know about?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "Thank you for being here! Today, I would like to discuss a scientific paper. Could you please provide me with the title or a brief summary of the paper you would like to discuss?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "Sure! The paper I would like to discuss is titled \"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION\". This paper focuses on improving the performance of language models in long context scenarios by using a technique called prompt compression.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "That sounds like an interesting paper! To start our discussion, could you explain what language models are and how they are typically used in natural language processing?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_engine *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"What are language models and how are they used in natural language processing?\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_engine...\u001b[0m\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_engine\" *****\u001b[0m\n",
      "['Preprint. Under review.\\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\\nprompts for accelerated inference of large language models. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language Processing . Association for Computational\\nLinguistics, December 2023a. URL https://arxiv.org/abs/2310.05736 .\\nZhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, and Jimmy Lin. “low-\\nresource” text classification: A parameter-free classification method with compressors. In Find-\\nings of the Association for Computational Linguistics: ACL 2023 , pp. 6810–6828, Toronto,\\nCanada, 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.\\n426.', '(2) How efficient is LongLLMLingua?\\nImplementation Details In this paper, we use GPT-3.5-Turbo-06134and LongChat-13B-16k as\\nthe target LLMs, both accessible via OpenAI5and HuggingFace6. To ensure stable and reproducible\\nresults, we employ greedy decoding and set the temperature to 0 in all experiments. For the small\\nlanguage models used for compression, we apply LLaMA-2-7B-Chat7, which has been aligned by\\nsupervised fine-tuning and RLHF. We implement our approach with PyTorch 1.13.1 and Hugging-\\nFace Transformers. We set up hyperparameters following LLMLingua except for the segment size\\nused in iterative token-level compression set to 200 here. More details are provided in Appendix B.\\nDataset & Evaluation Metric We use NaturalQuestions for the multi-document QA task, and use\\nLongBench and ZeroSCROLLS for general long context scenarios.', 'Addition-\\nally, when compressing prompts of ∼10k tokens at a compression rate of 2x-10x,\\nLongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.1\\n1 I NTRODUCTION\\nChatGPT and other large language models (LLMs) have revolutionized user-oriented language tech-\\nnologies and are serving as crucial components in more and more applications. Carefully designing\\nprompts is necessary to achieve better performance in specific downstream tasks. The commonly\\nused technologies such as In-Context Learning (ICL) (Dong et al., 2023), Retrieval Augment Gener-\\nation (RAG) (Lewis et al., 2020), and Agent (Park et al., 2023) are driving prompts to be increasingly\\nlonger, even reaching thousands of tokens. Scenarios such as multi-document question answering,\\ncode completion, and document summarization also necessitate the processing of long contexts.', '3 P RELIMINARY : LLML INGUA\\nLLMLingua (Jiang et al., 2023a) uses a small language model MSto calculate the perplexity of\\neach token in the original prompt and then removes tokens with lower perplexities. The rationale\\nbehind this approach is that tokens with lower perplexities contribute less to the overall entropy gain\\nof the language model, so removing them will have a relatively minor impact on the LLM’s compre-\\nhension of the context. LLMLiungua consists of three components: a budget controller, an iterative\\ntoken-level prompt compression algorithm, and a distribution alignment mechanism, as shown by\\nItalic texts in Figure 2. The budget controller allocates different compression ratios to the various\\ncomponents in the original prompt ( i.e., instruction, demonstrations, question), and performs coarse-\\ngrained compression at the demonstration level. The intermediate results are divided into segments\\nand the token-level compression is then performed segment by segment, with the perplexity of each\\ntoken conditioned on previous compressed segments calculated by MS.']\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "Language models are algorithms that are trained to understand and generate human language. They are a key component of natural language processing (NLP) and are used for a variety of tasks such as text generation, machine translation, sentiment analysis, and question answering.\n",
      "\n",
      "In NLP, language models are typically trained on large datasets of text, such as books, articles, and websites, to learn the statistical patterns and relationships between words. This allows them to generate coherent and contextually appropriate text.\n",
      "\n",
      "Language models are used in NLP by providing them with a prompt or a starting sentence, and then generating the rest of the text based on the learned patterns. For example, if you give a language model the prompt \"Once upon a time\", it can generate a complete story based on that prompt.\n",
      "\n",
      "These models have become increasingly powerful with advancements in deep learning and have been used in various applications to improve human-computer interactions and automate tasks that involve understanding and generating human language.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "Thank you for the explanation! Now, let's dive into the specific paper you mentioned, \"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION\". Could you explain what the term \"long context scenarios\" refers to in the context of language models?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_engine *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"What are long context scenarios in the context of language models?\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_engine...\u001b[0m\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_engine\" *****\u001b[0m\n",
      "['Preprint. Under review.\\nLongLLMLingua : A CCELERATING AND ENHANCING\\nLLM S IN LONG CONTEXT SCENARIOS VIA PROMPT\\nCOMPRESSION\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo,\\nDongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu\\nMicrosoft Corporation\\n{hjiang,qianhuiwu,xufang.luo }@microsoft.com\\nABSTRACT\\nIn long context scenarios, large language models (LLMs) face three main chal-\\nlenges: higher computational/financial cost, longer latency, and inferior perfor-\\nmance. Some studies reveal that the performance of LLMs depends on both the\\ndensity and the position of the key information (question relevant) in the input\\nprompt. Inspired by these findings, we propose LongLLMLingua for prompt com-\\npression towards improving LLMs’ perception of the key information to simulta-\\nneously address the three challenges.', 'Addition-\\nally, when compressing prompts of ∼10k tokens at a compression rate of 2x-10x,\\nLongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.1\\n1 I NTRODUCTION\\nChatGPT and other large language models (LLMs) have revolutionized user-oriented language tech-\\nnologies and are serving as crucial components in more and more applications. Carefully designing\\nprompts is necessary to achieve better performance in specific downstream tasks. The commonly\\nused technologies such as In-Context Learning (ICL) (Dong et al., 2023), Retrieval Augment Gener-\\nation (RAG) (Lewis et al., 2020), and Agent (Park et al., 2023) are driving prompts to be increasingly\\nlonger, even reaching thousands of tokens. Scenarios such as multi-document question answering,\\ncode completion, and document summarization also necessitate the processing of long contexts.', 'We conduct evaluation on a wide range of\\nlong context scenarios including single-/multi-document QA, few-shot learning,\\nsummarization, synthetic tasks, and code completion. The experimental results\\nshow that LongLLMLingua compressed prompt can derive higher performance\\nwith much less cost. The latency of the end-to-end system is also reduced. For\\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance\\nboost of up to 17.1% over the original prompt with ∼4x fewer tokens as input\\nto GPT-3.5-Turbo. It can derive cost savings of $28.5 and $27.4 per 1,000 sam-\\nples from the LongBench and ZeroScrolls benchmark, respectively.', '(2) How efficient is LongLLMLingua?\\nImplementation Details In this paper, we use GPT-3.5-Turbo-06134and LongChat-13B-16k as\\nthe target LLMs, both accessible via OpenAI5and HuggingFace6. To ensure stable and reproducible\\nresults, we employ greedy decoding and set the temperature to 0 in all experiments. For the small\\nlanguage models used for compression, we apply LLaMA-2-7B-Chat7, which has been aligned by\\nsupervised fine-tuning and RLHF. We implement our approach with PyTorch 1.13.1 and Hugging-\\nFace Transformers. We set up hyperparameters following LLMLingua except for the segment size\\nused in iterative token-level compression set to 200 here. More details are provided in Appendix B.\\nDataset & Evaluation Metric We use NaturalQuestions for the multi-document QA task, and use\\nLongBench and ZeroSCROLLS for general long context scenarios.']\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "In the context of language models, \"long context scenarios\" refer to situations where the input prompt given to the language model contains a significant amount of text or information. These scenarios involve prompts that are longer in length, sometimes reaching thousands of tokens.\n",
      "\n",
      "Long context scenarios can arise in various applications, such as multi-document question answering, code completion, document summarization, and other tasks that require processing and understanding of lengthy contexts.\n",
      "\n",
      "The challenge with long context scenarios is that they can lead to higher computational and financial costs, longer latency in generating responses, and potentially lower performance of the language model. This is because the model needs to process and comprehend a larger amount of text, which can be time-consuming and resource-intensive.\n",
      "\n",
      "The paper \"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION\" proposes a technique called prompt compression to address these challenges and improve the performance of language models in long context scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "Thank you for the explanation. Prompt compression sounds like an interesting technique. Could you provide more details about how prompt compression works and how it helps improve the performance of language models in long context scenarios?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_engine *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"How does prompt compression work and how does it improve the performance of language models in long context scenarios?\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_engine...\u001b[0m\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_engine\" *****\u001b[0m\n",
      "['Preprint. Under review.\\nLongLLMLingua : A CCELERATING AND ENHANCING\\nLLM S IN LONG CONTEXT SCENARIOS VIA PROMPT\\nCOMPRESSION\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo,\\nDongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu\\nMicrosoft Corporation\\n{hjiang,qianhuiwu,xufang.luo }@microsoft.com\\nABSTRACT\\nIn long context scenarios, large language models (LLMs) face three main chal-\\nlenges: higher computational/financial cost, longer latency, and inferior perfor-\\nmance. Some studies reveal that the performance of LLMs depends on both the\\ndensity and the position of the key information (question relevant) in the input\\nprompt. Inspired by these findings, we propose LongLLMLingua for prompt com-\\npression towards improving LLMs’ perception of the key information to simulta-\\nneously address the three challenges.', 'To\\nreduce information loss in the middle, we introduce a document reordering mechanism.\\nInspired by these observations, we propose LongLLMLingua to address the three challenges. Specif-\\nically, we use the advanced while efficient LLMLingua (Jiang et al., 2023a) as our backbone frame-\\nwork for prompt compression to address the first two challenges, i.e., reduce cost and latency. How-\\never, in the case of long contexts, the distribution of question-relevant key information in the prompt\\nis generally sparse. Existing prompt compression methods like LLMLingua (Jiang et al., 2023a) and\\nSelective-Context (Li, 2023) that do not consider the content of the question during compression may\\nretain too much noisy information in the compressed results, leading to inferior performance. In this\\npaper, LongLLMLingua is designed to enhance LLM’s perception of key information (relevant to\\nthe question) in the prompt, so that the third challenge of inferior performance in long context sce-\\nnarios could be addressed.', 'It is shown that our prompt compres-\\nsion system does accelerate the overall inference. As\\nthe compression rate increases, the acceleration ef-\\nfect becomes more pronounced. It is worth mentioning that in scenarios with longer API cost time,\\nthe actual absolute time saved by LongLLMLingua can be more significant.\\n6 R ELATED WORKS\\nLong Context for LLMs . Recent research has focused on expanding the window size of LLMs.\\nMain approaches include: (1) Staged pre-training (Nijkamp et al., 2023) which gradually increases\\nthe context window; (2) Modifying (Press et al., 2022) or interpolating position embeddings (Chen\\net al., 2023; Peng et al., 2023; Han et al., 2023); (3) Using linear or sparse attention mecha-\\nnisms (Ding et al., 2023; Sun et al., 2023); (4) Utilizing external memory modules for context\\nstorage (Bertsch et al., 2023; Tworkowski et al., 2023).', 'Addition-\\nally, when compressing prompts of ∼10k tokens at a compression rate of 2x-10x,\\nLongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.1\\n1 I NTRODUCTION\\nChatGPT and other large language models (LLMs) have revolutionized user-oriented language tech-\\nnologies and are serving as crucial components in more and more applications. Carefully designing\\nprompts is necessary to achieve better performance in specific downstream tasks. The commonly\\nused technologies such as In-Context Learning (ICL) (Dong et al., 2023), Retrieval Augment Gener-\\nation (RAG) (Lewis et al., 2020), and Agent (Park et al., 2023) are driving prompts to be increasingly\\nlonger, even reaching thousands of tokens. Scenarios such as multi-document question answering,\\ncode completion, and document summarization also necessitate the processing of long contexts.']\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "Prompt compression is a technique used to improve the performance of language models in long context scenarios. In the context of the paper \"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION,\" prompt compression is specifically designed to address the challenges faced by large language models (LLMs) in long context scenarios, which include higher computational/financial costs, longer latency, and inferior performance.\n",
      "\n",
      "The goal of prompt compression is to reduce the length of the input prompt while preserving the key information that is relevant to the task or question at hand. By compressing the prompt, the computational and financial costs associated with processing long contexts can be reduced, and the latency in generating responses can be improved.\n",
      "\n",
      "In the case of long contexts, the distribution of question-relevant key information in the prompt is often sparse. Existing prompt compression methods may retain too much noisy information in the compressed results, leading to inferior performance. The proposed technique, LongLLMLingua, aims to enhance the language model's perception of the key information in the prompt, specifically the information relevant to the question. This helps improve the model's understanding and performance in long context scenarios.\n",
      "\n",
      "By compressing the prompt while maintaining the essential information, LongLLMLingua can simultaneously address the challenges of higher computational/financial costs, longer latency, and inferior performance in long context scenarios. The experimental results in the paper show that LongLLMLingua can achieve higher performance with much less cost and reduce the end-to-end latency of the system.\n",
      "\n",
      "In summary, prompt compression is a technique that reduces the length of the input prompt while preserving the relevant information, thereby improving the performance of language models in long context scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "Thank you for the detailed explanation! It's fascinating to see how prompt compression can address the challenges faced by language models in long context scenarios. In the paper, \"LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION,\" did the authors provide any specific methods or algorithms for implementing prompt compression?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minterviewee\u001b[0m (to interviewer):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_engine *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"What methods or algorithms are used for implementing prompt compression in the paper 'LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION'?\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_engine...\u001b[0m\n",
      "\u001b[33minterviewer\u001b[0m (to interviewee):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_engine\" *****\u001b[0m\n",
      "['Recently, Jiang et al. (2023b)) proposed an unsupervised dense retrieval method that\\nleverages traditional compression algorithms, such as gzip, and k-nearest neighbors.\\nPrompt Compression Methods can be grouped into three main categories: (1) Token prun-\\ning (Goyal et al., 2020; Kim & Cho, 2021; Modarressi et al., 2022) and token merging (Bolya\\net al., 2023), which need model fine-tuning or intermediate results during inference and have been\\nused with BERT-scale models. (2) Soft prompt tuning methods like GIST (Mu et al., 2023), Au-\\ntoCompressor (Chevalier et al., 2023), and ICAE (Ge et al., 2023), which require LLMs’ parame-\\nter fine-tuning, making them suitable for specific domains but not directly applicable to black-box\\nLLMs.', 'To\\nreduce information loss in the middle, we introduce a document reordering mechanism.\\nInspired by these observations, we propose LongLLMLingua to address the three challenges. Specif-\\nically, we use the advanced while efficient LLMLingua (Jiang et al., 2023a) as our backbone frame-\\nwork for prompt compression to address the first two challenges, i.e., reduce cost and latency. How-\\never, in the case of long contexts, the distribution of question-relevant key information in the prompt\\nis generally sparse. Existing prompt compression methods like LLMLingua (Jiang et al., 2023a) and\\nSelective-Context (Li, 2023) that do not consider the content of the question during compression may\\nretain too much noisy information in the compressed results, leading to inferior performance. In this\\npaper, LongLLMLingua is designed to enhance LLM’s perception of key information (relevant to\\nthe question) in the prompt, so that the third challenge of inferior performance in long context sce-\\nnarios could be addressed.', '2023). Reorder: we reorder the documents with relevance metrics of\\ndifferent baselines as our document reordering strategy described in Sec. 4.2. In the case of OpenAI,\\nit corresponds to LongContextReorder in the LangChain framework (Chase, 2022). For results\\nreported under 1st to 20th, we do not use the reordering strategy for all methods.\\nLongLLMLingua coarse-grained compression. We discard sentences or paragraphs with low associ-\\nation until the compression constraint is met while keeping the original document order unchanged.\\n(ii) Compression-based Methods. We compare our approach with two state-of-art methods for\\nprompt compression, i.e., Selective Context (Li, 2023) and LLMLingua (Jiang et al., 2023a). Both\\nmethods employ LLaMA-2-7B-Chat as the small language model for compression.', 'Preprint. Under review.\\nLongLLMLingua : A CCELERATING AND ENHANCING\\nLLM S IN LONG CONTEXT SCENARIOS VIA PROMPT\\nCOMPRESSION\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo,\\nDongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu\\nMicrosoft Corporation\\n{hjiang,qianhuiwu,xufang.luo }@microsoft.com\\nABSTRACT\\nIn long context scenarios, large language models (LLMs) face three main chal-\\nlenges: higher computational/financial cost, longer latency, and inferior perfor-\\nmance. Some studies reveal that the performance of LLMs depends on both the\\ndensity and the position of the key information (question relevant) in the input\\nprompt. Inspired by these findings, we propose LongLLMLingua for prompt com-\\npression towards improving LLMs’ perception of the key information to simulta-\\nneously address the three challenges.']\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 4877 tokens (4823 in the messages, 54 in the functions). Please reduce the length of the messages or functions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/Chris_Pang/Developer/Code_Repository/llm_podgen/src/script_agent.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Chris_Pang/Developer/Code_Repository/llm_podgen/src/script_agent.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m interviewee\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Chris_Pang/Developer/Code_Repository/llm_podgen/src/script_agent.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     interviewer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Chris_Pang/Developer/Code_Repository/llm_podgen/src/script_agent.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mThank you for having me on the show, what do you want to know about?\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Chris_Pang/Developer/Code_Repository/llm_podgen/src/script_agent.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "    \u001b[0;31m[... skipping similar frames: ConversableAgent.send at line 334 (15 times), ConversableAgent.receive at line 464 (14 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    804\u001b[0m         context,\n\u001b[1;32m    805\u001b[0m         use_cache,\n\u001b[1;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    807\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/autogen/oai/completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[0;32m--> 222\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Code_Repository/llm_podgen/.venv/lib/python3.10/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 4877 tokens (4823 in the messages, 54 in the functions). Please reduce the length of the messages or functions."
     ]
    }
   ],
   "source": [
    "interviewee.initiate_chat(\n",
    "    interviewer,\n",
    "    message=\"Thank you for having me on the show, what do you want to know about?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
